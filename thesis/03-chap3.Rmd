---
chapter: 3
---

# Visual inference {#ch:lit}

<!--what is a linear mixed model? what are the residuals?
*why the visual inference is important?*
what are we going to diagnostic?
how can we diagnostic?
why we should use this diagnostic?
how does it works? where it will be used?
why the lmm is useful?-->

<!--ET: here consider making headers informative, interweaving various literature in a narrative -->

## Residual diagnostics

Residual diagnostics are common approach to infer the appropriateness of statistical models, however this is complicated in linear mixed models (LMMs) due to different types of residuals. There are three basic types of residuals for a linear mixed models, that is, marginal residuals $\boldsymbol{\hat{\xi}}$, conditional residuals $\mathbf{\hat{e}}$ and random effect residuals $\mathbf{Z\hat{b}}$ [@haslett2007three]. More specifically, a marginal residual is the difference between the observed data and the estimated marginal mean,
$$\boldsymbol{\hat{\xi}} = \mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}$$
A conditional residual is the difference between observed data and predicted value of the observation,
$$\mathbf{\hat{e}} = \mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{Z}\hat{\mathbf{b}}$$
A random effects residual is the difference between the predicted responses and the population average, $\mathbf{Z}\hat{\mathbf{b}}$.
@singer2017graphical summarised the areas of application for each type of residual defined by LMMs such as detecting the linearity of effects, assessing the normality and homoscedasticity of errors, checking outliers and accessing the covariance structure for individual subjects. There are various ways of judging deviations from normality for residual errors and for random effects, although it should be noted that none confirm normality and in many cases, normality is assumed to be granted (due to either the limitation of sample size or neglect for diagnosis). A common approach to infer normality is to use quantile-quantile (QQ) plot (@pinheiro2006mixed).
Marginal residuals are useful in checking the linearity of fixed effects, outlying observations and the covariance structure of $\boldsymbol\Omega_i$, while the conditional residuals can be used to detect the outlying observations, homoscedasticity and normality of the conditional errors. Moreover, the random effects can be applied to examine the outlying subjects and the normality of the random effects (@singer2017graphical). 

According to @hilden1995multilevel, a residual is confounded when it is dependent on the other types of errors. Since the conditional and random effects residual are not independent of one another, then the conditional residual may not be adequate to check the normality of conditional errors. They try to minimise the fraction of confounding for the $k$-th conditional residual, namely,
$$0 \leq \frac{\mathbf{u}_k^\top\mathbf{RQZ}\boldsymbol{\Gamma}\mathbf{Z}^\top\mathbf{QR}\mathbf{u}_k}{\mathbf{u}_k^\top\mathbf{RQR}\mathbf{u}_k} = 1 - \frac{\mathbf{u}_k^\top\mathbf{RQR}\mathbf{QR}\mathbf{u}_k}{\mathbf{u}_k^\top\mathbf{RQR}\mathbf{u}_k} \leq 1$$
where $\mathbf{u}_k$ is the $k$-th column of $\mathbf{I}_N$. Furthermore, @hilden1995multilevel proposed a linear transformation of the conditional residuals to get the ($n-q$) least confounded conditional residuals ($\mathbf{c}^\top_k \hat{\mathbf{e}}$). The least confounded residuals are given by,
$$\mathbf{c}^\top_k \hat{\mathbf{e}} = \lambda_k^{-1/2}\boldsymbol{\ell}_k^\top\mathbf{R}^{-1/2}\hat{\mathbf{e}} = \lambda_k^{1/2}\boldsymbol{\ell}_k^\top\mathbf{R}^{-1/2}\mathbf{y}, k = 1, 2, \ldots, N-p$$
where $1 \geq \lambda_1 \geq \ldots \geq \lambda_{N-p} > 0$ are the ordered values to $\boldsymbol{\Lambda}$, obtained from the spectral decomposition 
$$\mathbf{R}^{1/2} \mathbf{Q} \mathbf{R}^{1/2} = \mathbf{L}\boldsymbol{\Lambda}\mathbf{L}^\top, \mathbf{L}^\top \mathbf{L} = \mathbf{I}_{N-p}$$
and $\boldsymbol{\ell}_k$ represents the $k$-th column of $\mathbf{L}$. 
However, the linearly transformed residuals do not correspond to individual observations anymore. Besides, they may amplify the super-normality effect, which tends to look more normal than the underlying effects actually are [@schutzenmeister2012residual]. @schutzenmeister2012residual prefer to use the studentized conditional residuals. 

In @singer2017graphical's paper, they not only consider the residual analysis, they also explore the global influence analysis, such that, leverage analysis, case deletion analysis and local influence analysis based on details in @beckman1987diagnostics, @banerjee1997influence, @christensen1992case, @lesaffre1998local, among others.

In @loy2017model's literature, rather than three different types of residuals, they used two basic types of residuals of LMM that they termed level-1 (observation level) residuals and level-2 (group level) residuals. These are equivalent to the conditional residual and predicted random effects, respectively. Although both @singer2017graphical and @loy2017model use the conditional residuals to check the homogeneity of residual variance, @singer2017graphical used the standardised conditional residual versus the fitted value to detect the homoscedasticity of the conditional errors, whereas @loy2017model checked the relationship between conditional residual and one of the model's covariate. They also check the homogeneity of conditional residual variance between groups by comparing the conditional residual and grouping variable.
@loy2017model tested the linearity of conditional residual by plotting the explanatory variable versus the conditional errors. However, the linearity diagnostic only occurred in the fixed effect as @singer2017graphical mentioned by plotting the standardised marginal residuals against the fitted value.
Moreover, the distributional assessment of the random effect for @loy2017model is based on the Q-Q plot where the random effects follow a $t_3$ distribution. Whereas, plotting the $\chi^2_q$ QQ-plot for the Mahalannobis's distance between $\hat{\mathbf{b}_i}$ and $\mathbb{E} (\mathbf{b_i}) = 0$ is the way to check the normality of the random effects ($\mathbf{b}_i$) denoted by @singer2017graphical.
Rather than residual analysis, @loy2017model was motivated by model selection. They also used the residuals to detect the significance of a fixed effect by plotting a residual quantity from the model without the variable of interest with the values of that variable. They also detected if the model needs the random effect, if the random effects include both random intercept and random slope, and whether the random effects need to be correlated.

## Conventional hypothesis vs Visual inference

People always use classical statistic inference such as $p$-value or the $t$-statistic. But these methods only tell that there is a problem with the model. However, graphical diagnostic can not only tell the problem but also can tell the cause of the problem. 
@buja2009statistical outline the parallelism between quantitative testing and visual discovery and the steps for the visual inference. Both methods are started with the same set of hypotheses. As we all know, the conventional statistical inference makes up of 1) formulating the null and alternative hypotheses, 2) calculating the test statistic from the observed data, 3) comparing the test statistic based on a null distribution, and 4) making a decision for any rejections.
While, for visual inference, the test statistic is the plot of the data. And the plot of the data is compared with a set of plots of samples drawn from the null distribution. Accordingly, the null distribution of plots refers to the infinite collection of plots of null datasets sampled under the null hypothesis(@buja2009statistical, @majumder2013validation, @loy2017model). If the plot of data is distinguishable, that is selected as the most different, then the null hypothesis will be rejected. Meanwhile the visual discovery is based on the viewer's cognition when they see the plots. 

@buja2009statistical states the crucial difference between conventional hypothesis test and visual inference: quantitative testing needs the explicit prior specification of the features; by contrast, the range of visual discoveries under the exploration data analysis and model diagnostic is not pre-specified explicitly. However, this will result in rejection that will be known. Furthermore, the important divergence between conventional and visual testing is that lineups will almost rely on the viewers' evaluation (@majumder2013validation). 

Since a graphical representation of the data is chosen to display the strength of the parameter of interest $\theta$ in visual inference, a definition of *visual statistic* has been carried out by @buja2009statistical. In details, the plots are drawn from the data generated consistently with the null hypothesis are called null plots which denoted as $T(y_0)$ and data plot, $T(y)$, maps the actual data to the plot. Meanwhile, there is a tool called power of a visual test is developed by @majumder2013validation, which is useful in comparing the performance of visual inference and conventional test.

## Protocols

@buja2009statistical introduces two protocols for the inferential use of null plots based on null datasets drawn from null hypothesis, that is 'the Rorschach' and lineup.
The Rorschach is taken as the cognitive experimentation. To measure a data analyst's tendency to over interpret plots in which there is no or only obvious structure is the goal. Although it can be biased based on the analysts' knowledge, the aim of this training is to improve the awareness of the features when they detect. 
For the lineup, they asked the viewer to identify the most different plot among 20 plots where the data plot is randomly allocated among the 19 null plots (@buja2009statistical, @majumder2013validation, @loy2017model). Therefore, there is a one in 20 chance that the data plot will be pointed out. 

The steps are constructed by @loy2017model:

1. *Create lineup data*: Assuming that the proposed models are correct, we use the parametric bootstrap to simulate new responses, refit the model to these simulated responses, and extract the residuals of interest from the proposed model. For each lineup, this process is used to obtain $m-1=19$ simulated null datasets.
2. *Render lineups*: Draw small multiples of each of the null datasets and randomly insert the observed data among the nulls. Each plot is labelled by a number from 1 to $m$. These IDs are used for identification and later evaluation of results.
3. *Evaluate lineups*: Present the lineups to independent observers, instructing them to identify the plot most different from the set and asking them what feature led to their choice. Theses choices came in the form of four suggestions (in checkboxes) and one text box for a free-form answer.
4. *Evaluate the strength of evidence*: For a lineup of size $m=20$ that has been evaluated by $K$ independent observers, the number of evaluations of a lineup in which the observer identifies the data plot, $Y$, has a Visual distribution $V_{K,m, s =3}$ as defined by **Hofmann (2015)**

If the observers cannot find any distinguishable features, they may select one based on a random guess. Some observers may select the plot based on identifying features different to the rest (@buja2009statistical). @buja2009statistical also carries out the characteristic of lineup that not only choosing single plot among the lineups but also selecting the rank of the difference level of the plots. If this protocol is repeated to multiple independent observers, the $p$-value will be realized by tabulating the number of independent investigators chosen the data plot among the 19 null plots. Afterwards, the definition of $p$-values of lineup is implemented by @buja2009statistical and @majumder2013validation. For $K$ independent investigators, let $X$ be the number of observers picking the test statistic from the lineup. Under the null hypothesis, $X \sim B(K, 1/m)$ (for a lineup of size $m$). Hence, @majumder2013validation introduce the visual $p$-value of a lineup of a size $m$ evaluated by $K$ observers is given by
$$P(X \geq x),$$
where $x$ is the number of the observers pointing out the actual data plot. Then with the significance level $\alpha$, the author indicates decision rule that we can reject the null if out of $K$ observers at least $\chi_\alpha$ correctly identify the data plot, otherwise, fail to reject the null. While the lineup size will affect the probability that the viewer correctly points the data plot out proved by @majumder2013validation, such that with larger $m$, the probability of correctly distinguishing the data plot decreases. However, we often choose the lineup size as 20.

@loy2017model, @majumder2013validation, @buja2009statistical and etc. have used @turk2012amazon to run the visual inference experiments. In these experiments, the demographic information, such as age, gender, education level, will be asked. Furthermore, the time taken to complete questions for each observer is recorded and observers are asked to give the confidence score from 1 to 5 with weak to high (@majumder2013validation, @loy2017model). In order to test the ability of individuals, @majumder2013validation gives the easy lineup as the reference lineup. If they are able to identify the actual data plot, then the response for the reference lineup will be removed and all the following responses will be stored. If the answer for the reference lineup is wrong, then the all responses made by this observer will be removed.
Based on the examples that they give and also refer to the survey design (@dawes2000impact), the wording of the instruction might help the viewers to make wiser decisions.


<!--


### lineup protocol
One of the protocols, lineup, it randomly insert the plot of observed data among the 19 null plots which are generated from the reference distribution. 

1. Create lineup data: Assuming that the proposed models are correct, we use the parameteric bootstrap to simulate new responses, refit the model to these simulated responses, and extract the residuals of interest from the proposed model. For each lineup, this process is used to obtain $m-1=19$ simulated null datasets.
2. Render lineups: Draw samll multiples of each of the null datasets and randomly insert the observed data among the nulls. Each plot is labeled by a number from 1 to $m$. These IDs are used for identification and later evaluation of results.
3. Evaluate lineups: Present the lineups to independent observers, instructing them to identify the plot most different from the set and asking them what feature led to their choice. Thses choices came in the form of four suggestions (in checkboxes) and one text box for a free-form answer.
4. Evaluate the strength of evidence: For a lineup of size $m=20$ that has been evaluated by $K$ independent observers, the number of evaluations of a lineuo in which the observer identifies the data plot, $Y$, has a Visual distribution $V_{K,m, s =3} as defined by **Hofmann (2015)**

#### p-value of protocol from the Majumder

@majumder2013validation compute the $p$-value in the visual inference with the lineup: let $X$ be the random variable describing the number of independent observations, out of $N$, identifying the data plot. Then the $p$-value is the probability that at least $x$ observers chose the data plot, given the null hypothesis is true, if $X = x$ is the number of observers who chose the data plot from the lineup. Under the null hypothesis, the probability of choosing the true plot is $1/m$ where $m$ is the size of lineup, usually 20, and $X$ is distributed according to a distribution similar to a Binomial distribution $B_{N,1/m}$
#### Experimental setup
The visual inference also collect the information about the reason for the observers to make their decisions such as outlier, trend or asymmetry.

## Majumder
[@majumder2013validation] new research on formalizing statistical graphics with language characteristics makes it easier to abstractly define, compare, and contrast data plots. In section 2, it contains 6 definitions which including the visual statistic, lineup, $p$-value. - what is the $p$-value in this case?

They have test on the linear regression model 
$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1}X_{i2} + \ldots + \epsilon_i$$
where $\epsilon_i \sim iid N(0, \sigma^2)$, $i = 1, 2, 3, \ldots, n$. The covariates ($X_j, j = 1, 2, \ldots, p$) can be continuous or discrete. They introduce different types of visual test statistics for testing hypotheses such as to examine the effect of variable $X_j$ on $Y$, linearity, distributions and so on.

They publish the expriement on the Amazon's online web servie, Machanical Turk. The participants are asked to select the plot refering to the question, provide the reason why they choose, and tell the confidence level that they believe in their choice. At the meanwhile, each participant's age, gender, education level and geographic location are collected.
When they do the experiment, they have to estimate the observer's visual skills. For example, they gives the observers an easy lineup plot which is distinguishable and let them to choose the most different one. If the actual plot is correctly chosen by the participants, they keep the answers from their. 

They using the lineup protocol mainly focus on three data set, discrete, continuous and comtanimated. The comparison of the expected power of a visual test with the power of the conventional test is applied.

### Spherical random effects
If the variance component is estimated as zero or near zero, the covariance matrix of random effects is singular and its inverse is not defined.

Then introduce the lower triangular Cholesky factor of the covariance matirx of the random effects, for example, $V_b(\theta) = U_b(\theta)U_b(\theta)^\top$. Therefore, the spherical random effects as the vector $B^*$ that fullfills
$$B = U_b(\theta)B^*$$

The components of $B^*$ corresponding to zero variance component are assumed to have a value of zero.

$$Y = X\beta + ZU_b(\theta)B^* + U_\varepsilon \varepsilon^*$$
where replacing $\varepsilon$ with $\varepsilon^* = U^{-1}_e \varepsilon$ for $V_e = U_eU_e^\top$.

## Stephinie Project Protocol
They created three different version and replicate each version by four times. Each participant evaluate 12 lineups without same plot shown.

what is replication in the exprimental setup?

-->


