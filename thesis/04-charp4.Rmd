---
chapter: 4
#knit: "bookdown::render_book"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, cache=TRUE)
library(grid)
library(gridExtra)
library(png)
library(knitr)
library(kableExtra)
library(tibble)
```

# Methodology {#ch:method}

In this section, we describe (i) the three data sets used as a basis for the simulation settings; (ii) the construction of the lineups and (iii) the experimental set up for the survey. 

<!-- Our object is to find which residual (marginal versus conditional) is better in detecting the presence of outlying observation and which residual (conditional versus confounded) is better in checking the normality assumption. 

since our aim is to detect the residual diagnostics of linear mixed model.
-->

## Data sets

The three data sets that we used in this thesis are: reaction times in a sleep deprivation study data set in the `R` package `lme4` [@lme4]; autism study data set in `R` package `HLMdiag` [@HLMdiag]; and linguistic data set [@winter2013linear]. Each of these data sets have different characteristics. More specifically, the linguistic data set contains only categorical dependent variables; the variables in the sleep study data are all numerical variables except the subject level; and finally, the autism data contains a combination of numerical and categorical variables. We describe briefly about each data in the following sections with more detailed variable summaries in the Appendix chapter.

### Reaction times in a sleep deprivation study

Sleep deprivation and chronic sleep restriction endanger health, safety, productivity and quality of the life. To better understand the importance of sleep time, @belenky2003patterns conducted a test based on 66 observers with 3 hours, 5 hours, 7 hours and 9 hours daily time in bed (TIB). The sleep deprivation data set concentrates on 18 volunteers who spend 3 hours in bed per night. On day 0, the subjects have the normal amount of sleep. But for the rest of nine nights, 3 hours of sleep time is restricted to them. The output of 180 recorded responses in Reaction variable shows the average reaction time per day for each subject. As a result, those volunteers with 3-hour TIB, speed (mean and fastest 10% of responses) on the psychomotor vigilance task (PVT) declined [@belenky2003patterns]. 

### Autism study
<!-- will be replaced with other data set since it takes too much time -->
A prospective longitudinal study following 155 children between the ages of 2 and 13 who were diagnosed with either autism spectrum disorder or non-spectrum developmental delays at age 2 has been carried out by @anderson2009patterns to explore the changes in verbal and social abilities from childhood to adolescence. Assessments were made on the children at ages 2, 3, 5, 9, and 13, however, not all children were assessed at each age. Their age, gender (female or male) and race (white or non-white) were captured. Vineland Socialization Age Equivalent (VSAE) recorded the overall measurement of the child's social skill. Sequenced Inventory of Communication Development (SICD) assessed the expressive language development at aged 2, in which three groups were divided into low, median and high. In addition, the initial diagnostics at age 2 of each child has been recorded as autism or pervasive development disorder (pdd).

### Linguistic study

@winter2013linear introduced a linguistic study case on the voice pitch by identifying the gender, subjects, scenarios and attitudes. One of the scenarios was to ask different people for help. For example, one subject was asking the professor to help with polite attitude, or asking a peer for a favour on an informal condition.
The data contains 84 observations on the voice pitch (or frequency) from 6 subjects (3 females and 3 males) under 7 scenarios with 2 attitudes (informal or polite).

## Exploratory data analysis

_Exploratory data analysis_ (EDA) is a critical tool before fitting the model as this method can help us to identity the features of the raw data. In the sleep deprivation study data, we use the plot comparing the response and explanatory variable of interest using linear smoothers for each group. Figure \@ref(fig:sleep-eda) illustrates the random intercept and random slope in reaction time versus days from day 0 to day 9 corresponding to different subjects. Hence, we treat the explanatory variable as the designed matrix of fixed effect and the subjects as the designed matrix of random effect. To test the dependency of random effects, based on the method provided by @loy2017model, a scatter plot of predicted random effects with overlaid regression lines. The null plots are simulated from the model with the fact that random effects are not correlated and the data plot is made using the predicted random effects from original model fit to the observed data. While the regression lines show the degree of correlation between random effects. According to the result, although the correlation between random effects is small, it is hard to distinguish the data plot from null plots. Therefore, there is no need for the correlated random effect, that is, the random slope and random intercept are independent.

```{r sleep-eda, out.width="90%", fig.cap="Exploratory data analysis example in the sleep study case. It shows the random intercept and random slope with different subjects."}
knitr::include_graphics("figures/sleep_eda.png")
```


## Residuals Diagnostics

Residuals are used to examine model assumptions and to detect outliers and potentially influential data points.
As @singer2017graphical listed eight uses of residuals for diagnostic purposes, we focus on three residuals with two diagnostic purposes in this thesis: (1) checking presence of outlying observations based on marginal residuals as well as conditional residuals, and (2) detecting normality of conditional error according to conditional residuals and confounded residuals.
After fitting the data sets into the "best" model, we can estimate the fixed effect $\hat{\boldsymbol{\beta}}$ and predict the random effect $\hat{\mathbf{b}}$, with latter generated as a realisation from a normal distribution with zero mean and  variance-covariance matrix estimated from the data using either maximum likelihood or residual maximum likelihood [@patterson1971recovery]. Then we apply the method that @singer2017graphical stated to obtain these residuals and then acquire the plots.

Given the true variance of marginal residuals that $\mathbb{V}(\boldsymbol{\hat{\xi}}_i) = \boldsymbol{\Omega}_i - \mathbf{X}_i (\mathbf{X}_i^\top \boldsymbol{\Omega}_i^{-1}\mathbf{X}_i)^{-1}\mathbf{X}_i^\top$, the standardised marginal residuals can be generated as $\boldsymbol{\hat{\xi}}^*_{ij} = \boldsymbol{\hat{\xi}}_{ij}/[diag_j(\mathbb{\hat{V}}(\boldsymbol{\hat{\xi}}_i))]^{1/2}$, where $diag_j(\mathbb{\hat{V}}(\boldsymbol{\hat{\xi}}_i))$ is the $j$-th element of the main diagonal of $\mathbb{V}(\boldsymbol{\hat{\xi}}_i)$. 

With true variance of conditional residuals, $\mathbb{V}(\mathbf{\hat{e}}) = \mathbf{R}[\boldsymbol{\Omega}^{-1} - \boldsymbol{\Omega}^{-1}\mathbf{X}(\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\Omega}^{-1}]\mathbf{R} = \mathbf{RQR}$, @singer2017graphical suggested using the standardised conditional residuals, $\mathbf{\hat{e}}_{ij}^* = \mathbf{\hat{e}}_{ij}/diag_{ij}(\mathbf{\hat{R} \hat{Q} \hat{R}})$, where $diag_{ij}(\mathbf{\hat{R} \hat{Q} \hat{R}})$ representing the main diagonal element of $\mathbf{RQR}$ corresponding to the $j$-th observation of the $i$-th unit.

To detect outlying observations, we draw the element of the standardised marginal or conditional residuals versus the observation indices recommended by @singer2017graphical.
However, boxplot is applied for $\boldsymbol{\hat{\xi}}^*_{ij}$ or $\mathbf{\hat{e}}_{ij}^*$ versus the explanatory variables for the autism data set since the data size is big.

@hilden1995multilevel introduced a linear transformation of the conditional residuals which is called least confounded conditional residuals $c_k^\top \mathbf{\hat{e}}$ in order to minimise the fraction of confounding. They thought the ability to check for normality of the conditional errors increases. And we used the standardised least confounded conditional residuals $c_k^\top \mathbf{\hat{e}}^*$ by dividing $c_k^\top \mathbf{\hat{e}}$ by the squared root of the corresponding element in $\mathbf{C}\hat{\mathbf{R}}\hat{\mathbf{Q}}\hat{\mathbf{R}}\mathbf{C}^\top$.
Hence, we employ QQ plot of the standardised conditional residuals and standardised least confounded conditional residuals to check for normality. Whereas @pinheiro2006mixed considers QQ plot of $\hat{\mathbf{e}}/\hat{\sigma}$ for checking the normality of the conditional error, we stick with the method of generating the standardised conditional residuals with @singer2017graphical. 

## Experiment setup

On the basis of the data types that we have, three different versions are generated for each data type with four replicates:

<!-- Each data set are generated by three different versions with four replications. It constitutes 12 data plots in each data set with 2 different plot types, one for detecting outlying observations and another for checking normality. After fitting to the 'best' model from the raw data, we generate three different scenarios: -->

<!-- aim for 3 different cases is that: case 1 is hard to be distinguished, case 2 can be identified, case 3 is easily identified.-->

1. *Generated from the "best" model*: 
  
  The estimated random effect and error term are following a normal distribution with mean $\mathbf{0}$ and variance $\hat{\boldsymbol{\Gamma}}$ and $\hat{\mathbf{R}}$ respectively from the "best" model as well as the estimated fixed effect. The sample can be generated through $\mathbf{y}^* \sim \mathcal(\mathbf{X}\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\Omega}})$ where $\hat{\boldsymbol{\Omega}} = \mathbf{Z}\hat{\boldsymbol{\Gamma}}\mathbf{Z}' + \hat{\mathbf{R}}$. This is a scenario where the fitted model and the data generating process match and thus, we do not expect any graphical diagnostics to indicate the model is inappropriate. 
  <!-- Linear mixed model is generated as $y = X\beta +Zb + e$ where random effect ($b$) and error term ($e$) follow a normal distribution with mean 0 and variance $G$ and $R$ respectively. Fixed effect can be obtained from the model. Hence, a new response variable is generated. -->

2. *Added slight noises*:

  i) For sleep study case, within-unit correlated error terms have been introduced. For each subject, the error terms are correlated by value 25 which is round the standard deviation of the error term.
  ii) For linguistic case, we treat the error term under the $t$-distribution with $\nu = 1$ degree of freedom.
  iii) For autism case, we also treat the error term under the $t$-distribution with $\nu = 15$ degree of freedom at a random choice.

In this scenario, the fitted model has a mild mis-specification there is some small chance that some diagnostics may detect this. 

3. *Introduce extreme noises*:

  We randomly added some extreme values to response variable for particular subjects in each data set.
  
  i) For sleep study case, we randomly add 20% of mean value to 2 response values of each subject.
  ii) For linguistic case, we firstly create four tables with respect to the category of gender and attitude, that is female with polite attitude, female with informal attitude, male with polite attitude, and male with informal attitude. Then we alter 4 reaction time value in each table by adding 20% of mean value for the overall reaction time.
  iii) For autism case, we add 20% of the total median response values to roughly 20% of observations at random.

Therefore, 12 data sets are generated with corresponding 12 data plots constituting the residual plots for detecting the presence of outlying and checking the normality of conditional error.

In this scenario, the added extreme noises should not be modelled well by the fitted model so diagnostic assessments should indicate the inappropriateness of the model fit. 

<!--12 data plots are formed within each data type, where half of them are to check the normality and the others are to explore the outlying observations.-->

## Generating lineups

We use a parametric bootstrap approach to generate the null data for each lineup based on the simulated data with zero to some noise added as described in the previous section. More specifically:

1. We generate the vector of random effects from $\mathcal{N}(\mathbf{0}, \hat{\mathbf{G}})$ for each group, that is, generate $\mathbf{b}^*_i \sim \mathcal{N}(\mathbf{0}, \hat{\mathbf{G}})$ for $i = 1, 2, ..., n$ where $\hat{\mathbf{G}}$ is estimated from the fit of the "best" model to the simulated data.
2. Vector of conditional residuals is generated from $\mathcal{N}(\mathbf{0}, \hat{\mathbf{R}})$ for each group, that is, generate $\mathbf{e}^*_i \sim \mathcal{N}(\mathbf{0}, \hat{\mathbf{R}})$ for $i = 1, 2, ..., n$
3. Generate a bootstrap sample $\mathbf{y}^*_i$ from $\mathbf{y}_i = \mathbf{X}_i \hat{\boldsymbol{\beta}} + \mathbf{Z}_i \mathbf{b}^*_i + \mathbf{e}^*_i$ for each group $i = 1, 2, ...,n$
4. Refit the model to the bootstrap sample.
5. Repeat steps from 1 to 4 by 19 times.

From the null data, the null plots are produced which is consistent with the null hypothesis. Afterwards, we insert the data plot among 19 null plots to form the lineup protocol using the `nullabor` package [@nullabor]. We design the position of each data plot. According to Figure \@ref(fig:design), based on the "best" model, the data plots which point the presence of outlying observations of both marginal or conditional residuals are at the position 7 in the lineup for the first two replicates. Then, for replicates 3 and 4, the position of data plot is 13. The data plot which refers to check the normality of conditional error for either standardised conditional residual or least confounded conditional residual is in panel 9 of 20 plots as the basis of the first two replicates of first version. The position for next two replicates are in 4. The location of the data plot for the remainder two versions are followed the Figure \@ref(fig:design).

```{r design, out.width="90%", fig.cap="Experimental design diagram"}
knitr::include_graphics("figures/diagram.png")
```

<!-- we use the experiment design -->

## Survey through Shiny app

We download all the lineups to png files with first three letter for the data types (`aut`, `lin` and `slp`), followed by versions (`1`, `2`, and `3`) with replicates (`1`, `2`, `3`, and `4`). The diagnostic purposes are listed as the last letter where 1 for residual plots in marginal residuals, 2 represents the residual plots in conditional residuals, 3 stands for QQ plot under conditional residuals and 4 signals the QQ plot in confounded conditional residuals.
Using the Latin Squared Design method [@agricolae], we list the replicates 1 to 4 as the rows and four types of diagnostic plots are the column. A, B, C and D as the variates are listed in each cell without repeating in each row and column. Once the cell in the first row has been target, the corresponding row and column will be removed. And the matching diagnostic plot with version 1 will be presented to the observers for particular data type. This process will be iterated 4 times for each data type until we achieve the 12 lineups among 144 lineups for each individual.
These lineups are from 3 different data types with four diverse replicates for 2 different plot types but without restricting the versions. 

We use the `taipan` package [@taipan] to build our survey questions which include the demographic of audience and their responses. In the first tab, referred to Fig. \@ref(fig:surveyd), volunteer's name, gender, age, education level and whether they have studied econometrics or statistics are recorded. Next tab in Fig. \@ref(fig:surveyq), they are asked to choose the most different map from their decision and how certainty they are to choose that map. There is a process hit for them as the number of lineups they have been seen. They will iterate the process by 12 times by answering the same question but looking at different lineups. If their answers are less than 12 plots before submitting, there will be a hit popped up to reminder them to complete the survey before they leave. After they click the submit button, the final tab is shown up as the thank you page. All the responses made by each individual are securely stored into the google sheet based on the private link through `googlesheets4` package [@googlesheets4] with authenticating. Besides, we also acquire the unique identifier for them in case they double submitted. It is also useful when participant does not leave their names. The plot name with data type, version, replicates and plot type is also recorded which will be used in the further analysis. The time period that the participant takes are displayed as the reference.

```{r surveyd, out.width="90%", fig.cap="First tab in the survey created by shiny app. It contains the demographic information about the individuals."}
knitr::include_graphics("figures/geo_ss.png")
```

```{r surveyq, out.width="90%", fig.cap="Second tab in the survey created by shiny app. It includes the lineup and questions that we asked."}
knitr::include_graphics("figures/survey_ss.png")
```



# Results {#ch:result}

There are 54 individuals that attempt the survey. While, we removed two tests, people who are not sincerely doing the survey such that they responded all the lineups with the same choice or they just answered several questions. Therefore, we accepted the responses from 50 valid independent individuals with 600 observations.

Among 50 independent observers, 72% of them are female and the rest are male. 
All most half of the participants are aged at 18 to 24 years old and second big proportion is followed by the age range from 25 to 34. There are 18 viewers who achieved Bachelor degree while 20 of them had the Master degree and 8 got the Doctorate degree. Besides, 90% of them have studied econometrics or statistics before.


Based on the responses we got, we calculated the correctness from all these 50 observers.
Since we randomly select the versions of lineups based on our design, the total number of responses to each version is different. Referred to Fig. \@ref(fig:version), among 600 observations, 37% of the responses correspond to version 3, and version 3 obtains the most correct responses. 194 records are presented in version 2. The rest are listed in version 1 which has the least responses. There are fewer observers who can recognize the actual data plot in the version 1, which is in line with our expectations, because version 1 is simulated from the 'best' model, which may not have any obvious features.

```{r version, out.width="90%", fig.cap="Total amount of responses in each version"}
knitr::include_graphics("figures/version.png")
```

Table \@ref(tab:table) presents the summary of all lineups in evaluation. Three different data types (categorical, numerical and mixed) are combined in the result. We included four different purposes of lineups, where first two are detecting the outlying observation and the last two are checking the normality of the conditional errors and three different versions respectively. The ratio shown in the table is the division of number of observers who identify the actual data plot among the total number of participants in the same version and lineup. The visual $p$-value is calculated based on the method provided by @majumder2013validation.
From the results, the increase in the ratio for the version 2 compared with version 1 may indicate that for version 2, the data plot has certain features compared with the null plots. Among the 48 viewers, 9 selected the data plot in the quantile-quantile (QQ) plot to evaluate the normality of conditional errors based on the standardised conditional residual, thereby driving the visual $p$-value of 0.00055. This leads us to reject the null hypothesis that represents, overall, the conditional error does not obey a normal distribution.
Version 3 performs best among these three versions since it accounts for the largest proportion, and observers are able to point out the data plot among 19 null plots. The $p$-values of lineup are significant from all the results shown in the version 3, which is where we are able to reject the null hypotheses.

By comparing the ratios, we find that the standardised conditional residuals are more prominent than the standardised marginal residuals when checking the outlying observations. 21 out of 55 audiences are able to identify the actual data plot under the conditional residuals in the residual plot, while 12 out of 58 participants under the marginal residuals. The results shown are what we expected, because the conditional residuals squeeze the points which shows less information about the pattern but with more outstanding outlying observations. In addition, using quantile-quantile (QQ) plot to explore the normality of conditional errors, the performance of standardised conditional residuals is better than the performance of least confounded conditional residual, which is surprising.

<!-- table need to be fixed.. ugly -->

```{r table, echo=FALSE}
tribble(~Linups, ~'Version 1', ~'Version 2', ~'Version 3',
        "Marginal residual plot", '1/48', '3/44', '12/58 ***',
        "Condtional residual plot", '0/42', '4/53', '21/55 ***',
        "QQ plot for conditional residual", '3/49', '9/48 **', '26/53 ***',
        "QQ plot for confounded residual", '2/44', '5/49 .', '11/57 ***') %>% 
  kable(caption = "Overview of all lineup evaluations", booktabs = TRUE) %>% 
  add_footnote(
    c("Signif. codes: 0 $\\leq$ *** $\\leq$ 0.001 $\\leq$ ** $\\leq$ 0.01 $\\leq$ * $\\leq$ 0.05 $\\leq$ . $\\leq$ 0.1 $\\leq$ ' ' $\\leq$ 1"),
    notation = "symbol", escape = FALSE) 
```

Furthermore, we want to identify if the position of the data plot matters in Table \@ref(tab:position). We only consider the cases from version 2 and version 3 since they have more correct responses compared with version 1. The "1" means that the first two replicates, and "2" represents the last two replicates. Since the lineups are generated as 4 rows and 5 columns by label 1 to 20 from left to right. The residual plot in version 2, the data plot is located at 5 for first and second replicates while at 15 for the rest two replicates. For the version 2 in QQ plot, the data plot is allocated at 8 and 20 respectively. From the result, we can imply that if the data plot is at the corner, then it seems to be easier to be identified. Besides, if the data plot on the first row, it will be picked out quicker for example in the version 3. The data plot of the residual plots is in 15 and 2 respectively. And 11 and 6 are the two positions that apply in the QQ plot for version 3.
<!-- this result may got less evidence.. -->

```{r position}
tribble(~Lineups, ~V2.1, ~V2.2, ~V3.1, ~V3.2,
        "Marginal residual plot", "3/25", "0/19", "4/28 *", "8/30 ***",
        "Condtional residual plot", "3/29", "1/24", "9/26 ***", "12/29 ***",
        "QQ plot for conditional residual", "2/23", "7/25 ***", "15/29 ***", "11/24 ***",
        "QQ plot for confounded residual", "2/29", "3/20 .", "4/30 .", "7/27 ***") %>%
  kable(caption = "Summary table of position for lineup", booktabs = T)%>%
  add_footnote(
    c("Signif. codes: 0 $\\leq$ *** $\\leq$ 0.001 $\\leq$ ** $\\leq$ 0.01 $\\leq$ * $\\leq$ 0.05 $\\leq$ . $\\leq$ 0.1 $\\leq$ ' ' $\\leq$ 1"), 
               notation = "symbol",escape = FALSE) 
```

Based on version 3, the data type may also affect the results. As there are three data sets that have been used in this project, that is Autism case, Sleep study case, and Linguistic case. Each data set represents a particular data type, where linguistic case is the representative of categorical data type, numerical data type belongs to sleep study case and the rest, autism is the mixed data type. From the Table \@ref(tab:data-type), it shows that the mixed data type behaves well than the others, as the observers are able to recognize the data plot at the most proportion. Nevertheless, the categorical data types perform least as there are few people point out the actual data plot even in the version 3. In addition, among the mixed data type, it is better to use the standardised conditional residual to detect the outlying observations than based on the standardised marginal residual. Otherwise, the QQ plot applied in least confounded conditional residuals for conditional error model checking is harder to identify the data plot compared with standardised conditional residuals.

```{r data-type}
tribble(~"Lineups", ~"Categorical", ~"Numerical", ~"Mixed",
        "Marginal residual plot", "2/19", "2/21", "8/18 ***",
        "Condtional residual plot", "4/19 *", "3/19 .", "14/17 ***",
        "QQ plot for conditional residual", "0/11", "12/21 ***", "14/21 ***",
        "QQ plot for confounded residual", "1/14", "2/17 .", "8/26 ***") %>%
  kable(caption = "Summary table of position for lineup", booktabs = T) %>%
  add_footnote(
    c("Signif. codes: 0 $\\leq$ *** $\\leq$ 0.001 $\\leq$ ** $\\leq$ 0.01 $\\leq$ * $\\leq$ 0.05 $\\leq$ . $\\leq$ 0.1 $\\leq$ ' ' $\\leq$ 1"), 
    notation = "symbol", escape = FALSE)
```
<!-- certainty -->

We asked the independent viewers to record their confidence level when they choose the data that is the most different one. 1 means very uncertain, while 5 expresses very certain. From Fig. \@ref(fig:certainty), it is interesting to find that, for all three versions and data types, the neutral confidence level (3) has the biggest proportion. However, there is a dramatic increase from version 2 to version 3 with more amounts on higher confidence levels. Especially, in mixed data type, more observers are very certain when they make the choices. Furthermore, the correctness rates are rising from version 1 to version 3. Even they are not sure when they read the lineups, the data plot can be told from the null plots. In addition, with the confidence level from neutral above, there is an excellent ratio of identifying the data plot according to the mixed data type, which is followed by the numerical data type.

```{r certainty, out.width="90%", fig.cap="Level of certainty that made by observers when they reading the lineups"}
knitr::include_graphics("figures/certainty.png")
```
 
<!--plotting conditional residual against predictors or against observation indices performed reasonably well as methods for diagnosing violations of linear mixed model assumptions. -->

# Discussion {#ch:dis}

At this stage, there are some limitations that occurred when we built the survey and some unexpected things happened as well.

There are three data types for three different versions with four replications each, that is 144 lineups in total. If total number of observers is small, the result will become unreliable since some images may have few or even no responses, so we tried to encourage more individuals to join us.
Based on the survey that we created, we did not provide the reference lineup such that the visual ability of the individuals is not tested. It may result in the biases since we included the responses from those who are not well good at visual detecting. If the observers are able to identify the data plot from the reference lineup, their responses will be accepted otherwise, we will remove them.
If we can add the tolerance interval/area for QQ-plots and residual plots, it can give us more benefits on the result since it will be better in reflecting the null distribution for a specific diagnostic feature and improving objectivity for interpreting these plots.
In order to make the images clear for the viewers to read, we made the lineups as big as possible and put the questions on the sub panel side. The observers may lose patience because they need to scroll up and down the pages to see the whole picture of the lineups, make comparisons and answer the questions by repeating the process for 12 times. 
Also, there may some important hits are listed in the last panel of lineup. For some viewers, they will miss the information about the last 4 panels of lineups if they are not carefully reading the maps. 
Moreover, it is not clear for participants to see the text that "Please click the SUBMIT button at the top of the window" on the question panel for the last image. In addition, observers need to cross the whole page to click the SUBMIT button which is at the top right corner whereas the questions are listed at the bottom left. 
Furthermore, the position of the data plot may also matter. Since we designed the location of each data plot among the null plots for each case whereas the position for residual plots are the same as well as the QQ plots.
In the survey, we haven't asked the observers the reason of choosing the distinguishable plot among the lineup. This question may bring us some hit about the model.
Adding some words to explain the goal of the questions may improve the responses. Because we did not tell any information about the aim of this project, when the individuals see the residual plots which have been coloured by diverse subjects, they might treat the plots as the cluster plots which means that they were looking for the cluster pattern. 

# Conclusion {#ch:conc}

We have presented the graphical diagnosis using the lineups generated by simulation from the model, particular for the residual plots and quantile-quantile (QQ) plots based on three different type of residuals. The graphical approach is relatively new and involves working with human viewers and let them to choose the most different plot among the lineups.
From the result, we can conclude that using residual plot for identifying the outlying observations under the standardised conditional residuals is more efficient than for standardised marginal residuals. Standardised conditional residuals also perform better in diagnosing whether the conditional error follows a normal distribution than the standardised least confounded conditional residuals.

Lineup protocol is the alternative tool when we test the hypothesis. Instead of simply rejecting the null hypothesis, lineup also tells why we are rejecting the nulls. However, the graphical diagnostic does not only depend on the simulation methods, design of lineups, but also the observers. Theoretical power of the visual test may be greater than the conventional inference when the power of visual test increases with a large number of observers.
We remove all the contextual information in the lineups in the survey, such as title, axis and labels in order to make sure that observers are purely picking the most different plots among the null plots.

# Acknowledgments {-}

The author would like to thank Dr. Emi Tanaka for weekly meetings with suggestions and comments, Prof. Di Cook providing useful hits and Stephanie Kobakian supports the ideas for experimental setup and shiny app codes. We are thankful for the coding of residuals in the linear mixed model and the lineup structure provided by Adam Loy and Julio M. Singer.

The analysis of the work was completed in R [@team2019r] with the use of the following packages:

- lme4 [@lme4] and HLMdiag [@HLMdiag] were used for model fitting
- nullabor [@nullabor], ggplot2 [@ggplot2], dplyr [@dplyr], tidyverse [@tidyverse] for visualization and data analysis
- png [@png], grid [@grid] provided the image display
