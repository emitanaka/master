---
chapter: 1
knit: "bookdown::render_book"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, cache=TRUE)
# Load any R packages you need here
library(forecast)
library(ggplot2)
```

# Introduction {#ch:intro}

This is where you introduce the main ideas of your thesis, and an overview of the context and background.

Later chapters should be divided into coherent pieces describing your analysis. The final chapter should provide some concluding remarks, discussion, ideas for future research, and so on. Appendixes can contain additional material that don't fit into any chapters, but that you want to put on record. For example, additional tables, output, etc.

## Linear Mixed Model

The linear mixed model may be expressed as
$$\underset{(n_i \times 1)}{\mathbf{y}_i} = \underset{(n_i \times p)}{\mathbf{X}_i} \underset{(p\times 1)}{\boldsymbol{\beta}} + \underset{(n_i \times q)}{\mathbf{Z}_i}\underset{(q \times 1)}{\mathbf{b}_i} + \underset{(n_i \times 1)}{\mathbf{e}_i}$$
where there are $i = 1, 2, \ldots, g$ nonoverlapping groups.

$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Zb}+\mathbf{e}$$

The Gaussian LMM is coupled with
\begin{align*}
   \begin{bmatrix}
      \mathbf{b} \\ \mathbf{e}
   \end{bmatrix}
   \sim \mathcal{N}\left(\begin{bmatrix}\mathbf{0} \\ \mathbf{0} \end{bmatrix}, \begin{bmatrix} \mathbf{\Gamma} & \mathbf{0} \\ \mathbf{0} & \mathbf{R} \end{bmatrix}\right)
\end{align*}

Based on the model, the marginal distribution of $\mathbf{y}$ is
\begin{align*}
   \mathbf{y} \sim \mathcal{N}(\mathbf{X} \boldsymbol{\beta}, \mathbf{\Omega})
\end{align*}
where $\mathbf{\Omega} = \mathbf{Z\Gamma Z}^\top + \mathbf{R}$, and the conditional distribution of $\mathbf{y}$ given $\mathbf{b}$ is given by
\begin{align*}
   \mathbf{y}|\mathbf{b} \sim \mathcal{N}(\mathbf{X} \boldsymbol{\beta} + \mathbf{Zb},\mathbf{R})
\end{align*}

The joint density function of $\mathbf{y}$ and $\mathbf{b}$ is defined as
\begin{align*}
   f(\mathbf{y}, \mathbf{b}) &= g(\mathbf{y}|\mathbf{b})h(\mathbf{b}) \\
   &= const \times \exp\left(-\frac{1}{2}(\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb})^\top \mathbf{R}^{-1} (\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb})-\frac{1}{2}\mathbf{b}^\top \mathbf{\Gamma}^{-1} \mathbf{b}\right)
\end{align*}

\begin{align*}
    \frac{\partial f(\mathbf{y}, \boldsymbol{b})}{\partial \mathbf{\beta}} &\propto \frac{(\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb})^\top \mathbf{R}^{-1} (\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb}) + \mathbf{b}^\top \mathbf{\Gamma}^{-1} \mathbf{b}}{\partial \boldsymbol{\beta}} \\
    &\propto \frac{-\mathbf{y}^\top \mathbf{R}^{-1}\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y} + \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Zb} + \mathbf{b}^\top \mathbf{Z}^\top \mathbf{R}^{-1} \mathbf{X}\boldsymbol{\beta}}{\partial \boldsymbol{\beta}} \\
    &= -2 \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y} + 2\mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X}\boldsymbol{\beta} + 2\mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Zb} = 0 \\
    &\Rightarrow \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X}\boldsymbol{\beta} + \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Zb} = \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y}
\end{align*}

\begin{align*}
    \frac{\partial f(\mathbf{y}, \boldsymbol{b})}{\partial \mathbf{b}} &\propto \frac{(\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb})^\top \mathbf{R}^{-1} (\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb}) + \mathbf{b}^\top \mathbf{\Gamma}^{-1} \mathbf{b}}{\partial \mathbf{b}} \\
    &\propto \frac{-\mathbf{y}^\top \mathbf{R}^{-1}\mathbf{Zb} + \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{R}^{-1}\mathbf{Zb} -\mathbf{b^\top Z^\top R}^{-1}\mathbf{y} + \mathbf{b^\top Z^\top R}^{-1}\mathbf{X}\boldsymbol{\beta}+ \mathbf{b^\top Z^\top R}^{-1}\mathbf{Zb} + \mathbf{b^\top}\mathbf{\Gamma}^{-1}\mathbf{b}}{\partial \mathbf{b}} \\
    &= -2\mathbf{Z^\top R}^{-1}\mathbf{y} + 2\mathbf{Z^\top R}^{-1}\mathbf{X}\boldsymbol{\beta} + 2\mathbf{Z^\top R}^{-1}\mathbf{Zb} + 2\mathbf{\Gamma}^{-1}\mathbf{b} = 0\\
    &\Rightarrow \mathbf{Z^\top R}^{-1}\mathbf{X}\boldsymbol{\beta} + \mathbf{Z^\top R}^{-1}\mathbf{Zb} + \mathbf{\Gamma}^{-1}\mathbf{b} = \mathbf{Z^\top R}^{-1}\mathbf{y}
\end{align*}

\begin{align*}
    \begin{pmatrix} \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X} & \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Z} \\ \mathbf{Z^\top R}^{-1}\mathbf{X} & \mathbf{Z^\top R}^{-1}\mathbf{Z} +\mathbf{\Gamma}^{-1} 
    \end{pmatrix}
    \begin{pmatrix} \boldsymbol{\beta} \\ \mathbf{b} \end{pmatrix}
    =
    \begin{pmatrix}
    \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y} \\ \mathbf{Z^\top R}^{-1}\mathbf{y}
    \end{pmatrix}
\end{align*}

Write the coefficient matrix as:
\begin{align*}
    \mathbf{C} = \begin{bmatrix} \mathbf{C}_{XX} & \mathbf{C}_{XZ} \\ \mathbf{C}_{ZX} & \mathbf{C}_{ZZ}\end{bmatrix}
\end{align*}
Then, the model can be shown as,
\begin{align*}
    \mathbf{C}_{XX}\boldsymbol{\beta} + \mathbf{C}_{XZ} \mathbf{b} &= \mathbf{c}_{Xy} \\
    \mathbf{C}_{ZX}\boldsymbol{\beta} +\mathbf{C}_{ZZ} \mathbf{b} &= \mathbf{c}_{Zy}
\end{align*}
From the second equation, we can get $$\mathbf{b} = \mathbf{C}_{ZZ}^{-1}\mathbf{c}_{Zy} - \mathbf{C}_{ZZ}^{-1}\mathbf{C}_{ZX}\boldsymbol{\beta}$$

By substituting into the first equation,
$$(\mathbf{C}_{XX} - \mathbf{C}_{XZ}\mathbf{C}_{ZZ}^{-1}\mathbf{C}_{ZX})\boldsymbol{\beta} = \mathbf{c}_{Xy} - \mathbf{C}_{XZ}\mathbf{C}_{ZZ}^{-1}\mathbf{c}_{Zy}$$
where
\begin{align*}
    & \mathbf{C}_{XX} - \mathbf{C}_{XZ}\mathbf{C}_{ZZ}^{-1}\mathbf{C}_{ZX}\\
    &=  \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X} - \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Z}(\mathbf{Z^\top R}^{-1}\mathbf{Z} +\mathbf{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1}\mathbf{X} \\
    &=\mathbf{X}^\top (\mathbf{R}^{-1} - \mathbf{R}^{-1} \mathbf{Z}(\mathbf{Z^\top R}^{-1}\mathbf{Z} +\mathbf{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1})\mathbf{X} \\
    &= \mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X}
\end{align*}
Then,
\begin{align*}
    \boldsymbol{\beta} &= (\mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X})^{-1}(\mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y} -  \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Z}(\mathbf{Z^\top R}^{-1}\mathbf{Z} +\mathbf{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1}\mathbf{y}) \\
    &= (\mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X})^{-1}\mathbf{X}^\top(\mathbf{R}^{-1} - \mathbf{R}^{-1} \mathbf{Z}(\mathbf{Z^\top R}^{-1}\mathbf{Z} +\mathbf{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1})\mathbf{y} \\
    &= (\mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Omega}^{-1} \mathbf{y}
\end{align*}

Then, using the 
\begin{align*}
    \mathbf{b} &= \mathbf{C}_{ZZ}^{-1}\mathbf{c}_{Zy} - \mathbf{C}_{ZZ}^{-1}\mathbf{C}_{ZX} (\mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Omega}^{-1} \mathbf{y} \\
    &= \mathbf{C}_{ZZ}^{-1}(\mathbf{c}_{Zy} - \mathbf{C}_{ZX} (\mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Omega}^{-1} \mathbf{y}) \\
    &= (\mathbf{Z^\top R}^{-1}\mathbf{Z} +\mathbf{\Gamma}^{-1} )^{-1}(\mathbf{Z^\top R}^{-1}\mathbf{y} - \mathbf{Z^\top R}^{-1}\mathbf{X}(\mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Omega}^{-1} \mathbf{y}) \\
    &= (\mathbf{Z^\top R}^{-1}\mathbf{Z} +\mathbf{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1}(\mathbf{I} - \mathbf{X}(\mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Omega}^{-1} )\mathbf{y} \\
    &= \mathbf{\Gamma}\mathbf{Z^\top}\mathbf{\Omega}^{-1}(\mathbf{I} - \mathbf{X}(\mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Omega}^{-1} )\mathbf{y} \\
    &= \mathbf{\Gamma}\mathbf{Z^\top}\mathbf{\Omega}^{-1} \mathbf{Q}\mathbf{y}
\end{align*}
where $$\mathbf{Q} = \mathbf{\Omega}^{-1} - \mathbf{\Omega}^{-1}\mathbf{X}(\mathbf{X}^\top \mathbf{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{\Omega}^{-1}$$


## Literature review

what is a linear mixed model?
why the visual inference is important?
what are we going to diagnostic?
how can we diagnostic?
why we should use this diagnostic?
how does it works? where it will be used?
why the lmm is useful?

People always use classical statistic inference such as p-value or the test statistics. But why not using the graphical diagnostic. Graphical diagnostic can not only tell the problem but also can tell the cause of the problem.

Visual inference is parallel with the classical statistical inference. As we all knew, there are null hypothesis and alternative hypothesis in the classical statistical inference. However, they exist in the visual inference as well but with the plot rather than the statistical method.

residuals form the diagnostic core of the LME model

Model selection:
- fixed effect
- random effect

Model checking:
- Homogeneity of residual variance
- linearity
- Distributional assessment

* residual diagnostic of plots
  + marginal residuals, $\boldsymbol{\hat{\xi} = y - X \hat\beta}$
  + conditional residuals, $\hat{e} = y - X \hat\beta - Z \hat b$
  + random effects residuals, $Z \hat b$

* lineups

  The lineup protocal often contains 19 null plots and 

## Figures

Figure \ref{fig:deaths} shows time plots of the data we just loaded. Notice how figure captions and references work. Chunk names can be used as figure labels with `fig:` prefixed. Never manually type figure numbers, as they can change when you add or delete figures. This way, the figure numbering is always correct.

```{r deaths, message=FALSE, fig.cap="Quarterly sales, advertising and GDP data."}
autoplot(sales, facets=TRUE, ylab="", xlab="Year")
```

## Results from analyses

We can fit a dynamic regression model to the sales data.

```{r, echo=FALSE}
fit <- auto.arima(sales[,"Sales"], xreg=sales[,2:3], D=1)
if(!identical(fit$arma, c(1L,0L,0L,1L,4L,0L,1L)))
  stop("Model not ARIMA(1,0,0)(0,1,1)[4]")
#This is an example of how to put checks into your R code to warn you if something has gone wrong.
```

If $y_t$ denotes the sales in quarter $t$, $x_t$ denotes the corresponding advertising budget and $z_t$ denotes the GDP, then the resulting model is:
\begin{equation}
  y_t - y_{t-4} = \beta (x_t-x_{t-4}) + \gamma (z_t-z_{t-4}) + \theta_1 \varepsilon_{t-1} + \Theta_1 \varepsilon_{t-4} + \varepsilon_t
\end{equation}
where
$\beta = `r format(fit[['coef']]['AdBudget'], digits=2, nsmall=2)`$,
$\gamma = `r format(fit[['coef']]['GDP'], digits=2, nsmall=2)`$,
$\theta_1 = `r format(fit[['coef']]['ma1'], digits=2, nsmall=2)`$,
and
$\Theta_1 = `r format(fit[['coef']]['sma1'], digits=2, nsmall=2)`$.

## Tables

Let's assume future advertising spend and GDP are at the current levels. Then forecasts for the next year are given in Table \ref{tab:salesforecasts}.

```{r forecasts, results="asis"}
currentad <- rep(tail(sales[,"AdBudget"],1),4)
currentgdp <- rep(tail(sales[,"GDP"],1),4)
fc <- forecast(fit, xreg=cbind(AdBudget=currentad, GDP=currentgdp))
knitLatex::xTab(format(as.data.frame(fc), nsmall=1, digits=1),
   caption.bottom="Forecasts for the next year assuming Advertising budget and GDP are unchanged.",
   booktabs=TRUE, coldef='lrrrr',
   label='tab:salesforecasts')
```

Again, notice the use of labels and references to automatically generate table numbers. In this case, we need to generate the label ourselves.

The `knitLatex` package is useful for generating tables from R output. Other packages can do similar things including the `kable` function in `knitr` which is somewhat simpler but you have less control over the result. If you use `knitLatex` to generate tables, don't forget to include `results="asis"` in the chunk settings.

