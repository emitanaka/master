---
chapter: 2
#knit: "bookdown::render_book"
---

# Linear Mixed Model {#ch:lmm}

Linear mixed model (LMM) are more versatile in fitting complex, correlated data structures than linear models by taking into account the dependency structures between units. Linear mixed models are special case of what is generally known as mixed-effects model; named as such because the model consists a mix of fixed and random effects. Confusingly, mixed-effects models are known with a variety of other names such as panel data model (often in econometrics), hirarchical model or multi-level model (often in social science). To elucidate the model we are referring, the mathematical form of linear mixed models is presented next.

For $i = 1, 2, \ldots, n$ non-overlapping groups, a linear mixed model may be expressed as
$$\underset{(m_i \times 1)}{\mathbf{y}_i} = \underset{(m_i \times p)}{\mathbf{X}_i} \underset{(p\times 1)}{\boldsymbol{\beta}} + \underset{(m_i \times q)}{\mathbf{Z}_i}\underset{(q \times 1)}{\mathbf{b}_i} + \underset{(m_i \times 1)}{\mathbf{e}_i}$$

where $\mathbf{y}_i$ is a $m_i \times 1$ vector of response, $\mathbf{X}_i$ is a $m_i \times p$ fixed-effects design or regressor matrix, $\mathbf{Z}$ is a $m_i \times q$ known specification matrix corresponding to the random effects, $\mathbf{b}_i$ is a $q \times 1$ vector of random effects describing the between-group covariance structure, $\boldsymbol \beta$ is a $p \times 1$ vector of fixed effects governing the global mean structure, and $\mathbf e$ is an $m_i \times 1$ vector of random errors. The distributional assumptions are as follows: $\mathbf{b}_i \sim N(\boldsymbol{0}, \mathbf{G}_i)$; $\mathbf{e}_i \sim N(\boldsymbol{0}, \mathbf{R}_i)$; the random components $\mathbf{b}_i$ and $\mathbf{e}_i$ are independent. The matrices $\mathbf{G}_i$ and $\mathbf{R}_i$ are symmetrical, positive definite matrices. We assume $\mathbf{R}_i = \sigma^2\mathbf{I}_{m_i}$ throughout this thesis.

We can rewrite the model as
$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Zb}+\mathbf{e}$$
where $\mathbf y = (\mathbf y_1^\top, \ldots, \mathbf y_n^\top)^\top, \mathbf X = \oplus_{i=1}^n \mathbf X_i, Z = \oplus_{i=1}^n \mathbf Z_i, \mathbf b = (\mathbf b_1^\top, \ldots, \mathbf b_n^\top)^\top$ and $\mathbf e = (\mathbf e_1^\top, \ldots, \mathbf e_n^\top)^\top$. 

The key assumption of LMMs is that the residual errors and random effects are normally distributed, namely
\begin{align*}
   \begin{bmatrix}
      \mathbf{b} \\ \mathbf{e}
   \end{bmatrix}
   \sim \mathcal{N}_{M+N}\left(\begin{bmatrix}\mathbf{0_M} \\ \mathbf{0_N} \end{bmatrix}, \begin{bmatrix} \boldsymbol{\Gamma} & \mathbf{0_{M\times N}} \\ \mathbf{0_{M \times N}} & \mathbf{R} \end{bmatrix}\right)
\end{align*}
where $\mathbf N = \sum_{i=1}^n m_i$ and $\mathbf M = nq$.

Based on the model, the marginal distribution of $\mathbf{y}$ is
\begin{align*}
   \mathbf{y} \sim \mathcal{N}(\mathbf{X} \boldsymbol{\beta}, \boldsymbol{\Omega})
\end{align*}
with $\mathbb E(\mathbf y) = \boldsymbol{X\beta}$ and $\mathbb V(\mathbf y) = \boldsymbol{\Omega} = \boldsymbol{Z\Gamma Z}^\top + \mathbf{R}$, and the conditional distribution of $\mathbf{y}$ given $\mathbf{b}$ is given by
\begin{align*}
   \mathbf{y}|\mathbf{b} \sim \mathcal{N}(\mathbf{X} \boldsymbol{\beta} + \mathbf{Zb},\mathbf{R})
\end{align*}

Suppose we know the covariance matrices $\boldsymbol{\Gamma}$ and $\boldsymbol{R}$, the estimated best linear unbiased estimators (BLUE) of the fixed effects $\boldsymbol{\beta}$ and best linear predictors (BLUP) of the random effects $\mathbf{b}$ can be obtained.
Given the marginal distribution of $\mathbf{y}$ and the conditional distribution of $\mathbf{y}|\mathbf{b}$, the joint density function of $\mathbf{y}$ and $\mathbf{b}$ is defined as
\begin{align*}
   f(\mathbf{y}, \mathbf{b}) &= g(\mathbf{y}|\mathbf{b})h(\mathbf{b}) \\
   &= const \times \exp\left(-\frac{1}{2}(\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb})^\top \mathbf{R}^{-1} (\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb})-\frac{1}{2}\mathbf{b}^\top \boldsymbol{\Gamma}^{-1} \mathbf{b}\right).
\end{align*}

The parameters $\mathbf{b}$ and ${\boldsymbol{\beta}}$ can be estimated by maximising this function. Taking the first derivative and equating the derivatives to zero, the result is shown below.

\begin{align*}
    \frac{\partial f(\mathbf{y}, \boldsymbol{b})}{\partial \boldsymbol{\beta}} &\propto \frac{(\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb})^\top \mathbf{R}^{-1} (\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb}) + \mathbf{b}^\top \boldsymbol{\Gamma}^{-1} \mathbf{b}}{\partial \boldsymbol{\beta}} \\
    &\propto \frac{-\mathbf{y}^\top \mathbf{R}^{-1}\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y} + \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Zb} + \mathbf{b}^\top \mathbf{Z}^\top \mathbf{R}^{-1} \mathbf{X}\boldsymbol{\beta}}{\partial \boldsymbol{\beta}} \\
    &= -2 \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y} + 2\mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X}\boldsymbol{\beta} + 2\mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Zb} = 0 \\
    &\Rightarrow \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X}\boldsymbol{\beta} + \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Zb} = \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y}
\end{align*}

\begin{align*}
    \frac{\partial f(\mathbf{y}, \boldsymbol{b})}{\partial \mathbf{b}} &\propto \frac{(\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb})^\top \mathbf{R}^{-1} (\mathbf{y}-\mathbf{X} \boldsymbol{\beta} - \mathbf{Zb}) + \mathbf{b}^\top \boldsymbol{\Gamma}^{-1} \mathbf{b}}{\partial \mathbf{b}} \\
    &\propto \frac{-\mathbf{y}^\top \mathbf{R}^{-1}\mathbf{Zb} + \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{R}^{-1}\mathbf{Zb} -\mathbf{b^\top Z^\top R}^{-1}\mathbf{y} + \mathbf{b^\top Z^\top R}^{-1}\mathbf{X}\boldsymbol{\beta}+ \mathbf{b^\top Z^\top R}^{-1}\mathbf{Zb} + \mathbf{b^\top}\boldsymbol{\Gamma}^{-1}\mathbf{b}}{\partial \mathbf{b}} \\
    &= -2\mathbf{Z^\top R}^{-1}\mathbf{y} + 2\mathbf{Z^\top R}^{-1}\mathbf{X}\boldsymbol{\beta} + 2\mathbf{Z^\top R}^{-1}\mathbf{Zb} + 2\boldsymbol{\Gamma}^{-1}\mathbf{b} = 0\\
    &\Rightarrow \mathbf{Z^\top R}^{-1}\mathbf{X}\boldsymbol{\beta} + \mathbf{Z^\top R}^{-1}\mathbf{Zb} + \boldsymbol{\Gamma}^{-1}\mathbf{b} = \mathbf{Z^\top R}^{-1}\mathbf{y}
\end{align*}

Based on the two equations above, the mixed model equations (MME), which first proposed by @henderson1973sire, is listed as:
\begin{align*}
    \begin{pmatrix} \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X} & \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Z} \\ \mathbf{Z^\top R}^{-1}\mathbf{X} & \mathbf{Z^\top R}^{-1}\mathbf{Z} +\boldsymbol{\Gamma}^{-1} 
    \end{pmatrix}
    \begin{pmatrix} \hat{\boldsymbol{\beta}} \\ \hat{\boldsymbol{b}} \end{pmatrix}
    =
    \begin{pmatrix}
    \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y} \\ \mathbf{Z^\top R}^{-1}\mathbf{y}
    \end{pmatrix}
\end{align*}

And $\hat{\boldsymbol{\beta}}$ and $\hat{\boldsymbol{b}}$ refer to as "mixed model solutions". However, the question is that how we can get the results. Next we will explain the procedure of solving the question.

Based on the method provided by @smith1999multiplicative, we write the coefficient matrix as:

\begin{align*}
    \mathbf{C} = \begin{bmatrix} \mathbf{C}_{XX} & \mathbf{C}_{XZ} \\ \mathbf{C}_{ZX} & \mathbf{C}_{ZZ}\end{bmatrix}
\end{align*}
Then, the model can be shown as,
\begin{align*}
    \mathbf{C}_{XX}\boldsymbol{\beta} + \mathbf{C}_{XZ} \boldsymbol{b} &= \mathbf{c}_{Xy} \\
    \mathbf{C}_{ZX}\boldsymbol{\beta} +\mathbf{C}_{ZZ} \boldsymbol{b} &= \mathbf{c}_{Zy}
\end{align*}

where the $\mathbf{c}_{Xy} = \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y}$ and $\mathbf{c}_{Zy} = \mathbf{Z^\top R}^{-1}\mathbf{y}$.

From the second equation, we can get $$\mathbf{b} = \mathbf{C}_{ZZ}^{-1}\mathbf{c}_{Zy} - \mathbf{C}_{ZZ}^{-1}\mathbf{C}_{ZX}\boldsymbol{\beta}$$

By substituting $\mathbf{b}$ into the first equation,
$$(\mathbf{C}_{XX} - \mathbf{C}_{XZ}\mathbf{C}_{ZZ}^{-1}\mathbf{C}_{ZX})\boldsymbol{\beta} = \mathbf{c}_{Xy} - \mathbf{C}_{XZ}\mathbf{C}_{ZZ}^{-1}\mathbf{c}_{Zy}$$
where
\begin{align*}
    & \mathbf{C}_{XX} - \mathbf{C}_{XZ}\mathbf{C}_{ZZ}^{-1}\mathbf{C}_{ZX}\\
    &=  \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{X} - \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Z}(\mathbf{Z^\top R}^{-1}\mathbf{Z} +\boldsymbol{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1}\mathbf{X} \\
    &=\mathbf{X}^\top (\mathbf{R}^{-1} - \mathbf{R}^{-1} \mathbf{Z}(\mathbf{Z^\top R}^{-1}\mathbf{Z} +\boldsymbol{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1})\mathbf{X} \\
    &= \mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X}
\end{align*}
Then the best linear unbiased estimators (BLUE) of the fixed effects $\boldsymbol{\beta}$,
\begin{align*}
    \hat{\boldsymbol{\beta}} &= (\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1}(\mathbf{X}^\top \mathbf{R}^{-1} \mathbf{y} -  \mathbf{X}^\top \mathbf{R}^{-1} \mathbf{Z}(\mathbf{Z^\top R}^{-1}\mathbf{Z} +\boldsymbol{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1}\mathbf{y}) \\
    &= (\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1}\mathbf{X}^\top(\mathbf{R}^{-1} - \mathbf{R}^{-1} \mathbf{Z}(\mathbf{Z^\top R}^{-1}\mathbf{Z} +\boldsymbol{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1})\mathbf{y} \\
    &= (\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1} \mathbf{y}
\end{align*}

Substituting the result for $\hat{\boldsymbol{\beta}}$ into $\mathbf{b} = \mathbf{C}_{ZZ}^{-1}\mathbf{c}_{Zy} - \mathbf{C}_{ZZ}^{-1}\mathbf{C}_{ZX}\boldsymbol{\beta}$, the best linear unbiased predictors (BLUP) of the random effects $\mathbf{b}$ is shown as,
\begin{align*}
    \hat{\mathbf{b}} &= \mathbf{C}_{ZZ}^{-1}\mathbf{c}_{Zy} - \mathbf{C}_{ZZ}^{-1}\mathbf{C}_{ZX} (\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1} \mathbf{y} \\
    &= \mathbf{C}_{ZZ}^{-1}(\mathbf{c}_{Zy} - \mathbf{C}_{ZX} (\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1} \mathbf{y}) \\
    &= (\mathbf{Z^\top R}^{-1}\mathbf{Z} +\boldsymbol{\Gamma}^{-1} )^{-1}(\mathbf{Z^\top R}^{-1}\mathbf{y} - \mathbf{Z^\top R}^{-1}\mathbf{X}(\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1} \mathbf{y}) \\
    &= (\mathbf{Z^\top R}^{-1}\mathbf{Z} +\boldsymbol{\Gamma}^{-1} )^{-1}\mathbf{Z^\top R}^{-1}(\mathbf{I} - \mathbf{X}(\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1} )\mathbf{y} \\
    &= \boldsymbol{\Gamma}\mathbf{Z^\top}\boldsymbol{\Omega}^{-1}(\mathbf{I} - \mathbf{X}(\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1} )\mathbf{y} \\
    &= \boldsymbol{\Gamma}\mathbf{Z^\top}\mathbf{Q}\mathbf{y}
\end{align*}
where $$\mathbf{Q} = \boldsymbol{\Omega}^{-1} - \boldsymbol{\Omega}^{-1}\mathbf{X}(\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1}$$

This implies that 
$$\mathbb{E}(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}, \mathbb{V}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \Omega^{-1} \mathbf{X})^{-1}$$
$$\mathbb{E}(\hat{\mathbf{b}}) = \mathbf{0}, \mathbb{V}(\hat{\mathbf{b}}) = \boldsymbol{\Gamma}\mathbf{Z}^\top\mathbf{Q}\mathbf{Z}\boldsymbol{\Gamma}$$

Besides, the estimates of residuals are given by $\hat{\mathbf{e}} = \mathbf{R}\mathbf{Q}{\mathbf{y}}$, the proof is shown as,
\begin{align*}
  \hat{\mathbf{e}} &= \mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{Z}\hat{\mathbf{b}} \\
  &= \mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{Z}\boldsymbol{\Gamma}\mathbf{Z^\top}\boldsymbol{\Omega}^{-1}(\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}) \\
  &= (\mathbf{I} - \mathbf{Z}\boldsymbol{\Gamma}\mathbf{Z^\top}\boldsymbol{\Omega}^{-1})\mathbf{y} - (\mathbf{I} - \mathbf{Z}\boldsymbol{\Gamma}\mathbf{Z^\top}\boldsymbol{\Omega}^{-1})\mathbf{X}\hat{\boldsymbol{\beta}} \\
  &= (\mathbf{I} - \mathbf{Z}\boldsymbol{\Gamma}\mathbf{Z^\top}\boldsymbol{\Omega}^{-1})\mathbf{y} - (\mathbf{I} - \mathbf{Z}\boldsymbol{\Gamma}\mathbf{Z^\top}\boldsymbol{\Omega}^{-1})\mathbf{X}(\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1} \mathbf{y} \\
  &= (\mathbf{I} - \mathbf{Z}\boldsymbol{\Gamma}\mathbf{Z^\top}\boldsymbol{\Omega}^{-1})(\mathbf{I} - \mathbf{X}(\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1})\mathbf{y} \\
  &= (\boldsymbol{\Omega} - \mathbf{Z}\boldsymbol{\Gamma}\mathbf{Z^\top})(\boldsymbol{\Omega}^{-1} - \boldsymbol{\Omega}^{-1}\mathbf{X}(\mathbf{X}^\top \boldsymbol{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Omega}^{-1})\mathbf{y} \\
  &= \mathbf{R}\mathbf{Q}{\mathbf{y}}
\end{align*} 

@robinson1991blup gives an excellent introduction to the estimation of random effects and discusses its various fields of application.

After introducing the linear mixed model, there are several literatures explain the residuals applied in the model.
Similar to simple linear model, residuals are significant to diagnostics of linear mixed model. There are three various types of residuals based on @singer2017graphical, that is, marginal residuals $\boldsymbol{\hat{\xi}}$, conditional residuals $\mathbf{\hat{e}}$ and random effect residuals $\mathbf{Z\hat{b}}$. Besides, given that $\mathbf{\hat{e}} = \mathbf{R}\mathbf{Q}{\mathbf{e}} + \mathbf{R}\mathbf{Q}{\mathbf{Zb}}$, the conditional and random effects residuals may be confounded. It suggests that $\mathbf{\hat{e}}$ may not be adequate to check the normality of ${\mathbf{e}}$ because when $\mathbf{b}$ is grossly non-Gaussian, $\hat{\mathbf{e}}$ may not presented a Gaussian behaviour even when $\mathbf{e}$ is Gaussian. The further application of these residuals will be listed in Chapter \@ref(ch:lit) and Chapter \@ref(ch:method).

